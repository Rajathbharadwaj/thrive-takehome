# ğŸš€ Thrive DevOps Take-Home Challenge

Complete production-ready DevOps platform with Infrastructure as Code, CI/CD, monitoring, and advanced deployment strategies.

## ğŸŒŸ Quick Start - Live Demo

**ğŸ”— Application is live at: http://thrive.docrag.io/**

Try it now:
```bash
curl http://thrive.docrag.io/
curl http://thrive.docrag.io/health
```

## ğŸ“ Repository Structure

```
thrive-takehome/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ Dockerfile                          # Multi-stage Node.js container
â”œâ”€â”€ app/                               # Node.js application
â”‚   â”œâ”€â”€ server.js                      # Express.js web server
â”‚   â”œâ”€â”€ package.json                   # Dependencies
â”‚   â””â”€â”€ package-lock.json              # Lock file
â”œâ”€â”€ terraform/                         # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                        # EKS cluster, VPC, ALB
â”‚   â”œâ”€â”€ variables.tf                   # Configuration variables
â”‚   â”œâ”€â”€ outputs.tf                     # Resource outputs
â”‚   â”œâ”€â”€ ecr-lifecycle.tf              # ECR image retention
â”‚   â”œâ”€â”€ terraform.tfvars.example       # Configuration template
â”‚   â””â”€â”€ environments/                  # Environment configs
â”‚       â””â”€â”€ prod.tfvars               # Production settings
â”œâ”€â”€ k8s/                              # Kubernetes manifests
â”‚   â”œâ”€â”€ deployment.yaml               # Application deployment + service
â”‚   â”œâ”€â”€ ingress.yaml                  # ALB ingress config
â”‚   â”œâ”€â”€ namespace.yaml                # Namespace definition
â”‚   â””â”€â”€ hpa.yaml                      # Horizontal Pod Autoscaler
â”œâ”€â”€ .github/workflows/                # CI/CD pipeline
â”‚   â””â”€â”€ deploy-environments.yml       # Production CI/CD pipeline
â”œâ”€â”€ scripts/                          # Automation scripts
â”‚   â”œâ”€â”€ setup.sh                     # One-command deployment
â”‚   â”œâ”€â”€ cleanup.sh                   # Resource cleanup
â”‚   â”œâ”€â”€ start-dashboards.sh          # Monitoring dashboards
â”‚   â””â”€â”€ test-canary.sh               # Canary deployment test
â”œâ”€â”€ KUBECTL_COMMANDS.md              # Useful kubectl commands
â”œâ”€â”€ ARCHITECTURE.md                  # System architecture documentation
â”œâ”€â”€ CANARY_TESTING_GUIDE.md          # Canary deployment guide
â””â”€â”€ DEPLOYMENT_GUIDE.md              # Deployment procedures
```

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Internet"
        User[ğŸ‘¤ User]
    end
    
    subgraph "AWS Cloud"
        subgraph "Application Load Balancer"
            ALB[ğŸ”„ ALB<br/>Load Balancer]
        end
        
        subgraph "Amazon EKS Cluster"
            subgraph "Control Plane"
                API[ğŸ›ï¸ Kubernetes API]
            end
            
            subgraph "Worker Nodes (t3.small)"
                subgraph "Application Pods"
                    App1[ğŸ“¦ Node.js App]
                    App2[ğŸ“¦ Node.js App]
                end
                
                subgraph "Infrastructure Pods"
                    LBC[ğŸ”„ Load Balancer<br/>Controller]
                    Metrics[ğŸ“Š Prometheus<br/>Node Exporter]
                    Rollouts[ğŸš€ Argo Rollouts<br/>Controller]
                end
            end
        end
        
        subgraph "Monitoring Stack"
            Prometheus[ğŸ“ˆ Prometheus<br/>Metrics Server]
            Grafana[ğŸ“Š Grafana<br/>Dashboards]
        end
        
        subgraph "Storage & Registry"
            ECR[ğŸ“¦ Amazon ECR<br/>Container Registry]
            Secrets[ğŸ” AWS Secrets<br/>Manager]
        end
        
        subgraph "Networking"
            VPC[ğŸŒ VPC<br/>10.0.0.0/16]
            IGW[ğŸŒ Internet<br/>Gateway]
            NAT[ğŸ”„ NAT Gateway]
        end
        
        subgraph "Alerts & Notifications"
            CloudWatch[â˜ï¸ CloudWatch<br/>Alarms]
            SNS[ğŸ“§ SNS Topic<br/>Email Alerts]
        end
    end
    
    subgraph "GitHub Actions CI/CD"
        Pipeline[âš™ï¸ GitHub Actions<br/>Pipeline]
        OIDC[ğŸ” OIDC Provider<br/>Secure Auth]
    end
    
    %% User flow
    User --> ALB
    ALB --> App1
    ALB --> App2
    
    %% CI/CD flow
    Pipeline --> ECR
    Pipeline --> API
    OIDC --> Pipeline
    
    %% Monitoring flow
    App1 --> Prometheus
    App2 --> Prometheus
    Metrics --> Prometheus
    Prometheus --> Grafana
    Prometheus --> CloudWatch
    CloudWatch --> SNS
    
    %% Infrastructure connections
    App1 --> Secrets
    App2 --> Secrets
    LBC --> ALB
    Rollouts --> App1
    Rollouts --> App2
    
    %% Networking
    ALB --> IGW
    IGW --> User
    App1 --> NAT
    App2 --> NAT
    NAT --> IGW
    
    style User fill:#e1f5fe
    style ALB fill:#f3e5f5
    style App1 fill:#e8f5e8
    style App2 fill:#e8f5e8
    style Prometheus fill:#fff3e0
    style Grafana fill:#fff3e0
    style Pipeline fill:#f3e5f5
```

## ğŸš€ Deployment Instructions

### Prerequisites
- AWS CLI configured with Administrator access
- Git installed
- 5-10 minutes

### ğŸ†• Fresh AWS Account Setup

If deploying to a **brand new AWS account**, ensure you have:

1. **AWS CLI Configuration**:
   ```bash
   # Configure AWS CLI with Administrator access
   aws configure
   # AWS Access Key ID: [Your Access Key]
   # AWS Secret Access Key: [Your Secret Key]  
   # Default region name: us-east-1
   # Default output format: json
   ```

2. **Verify AWS Access**:
   ```bash
   # Test AWS connectivity
   aws sts get-caller-identity
   # Should return your account ID, user ARN, and user ID
   ```

3. **Account Limits** (No action needed - script handles this):
   - âœ… Default VPC will be used (no VPC limits hit)
   - âœ… All resources created with unique names
   - âœ… Script automatically handles resource conflicts

### Step-by-Step Deployment

#### 1. **Clone Repository**

```bash
# Clone repository
git clone https://github.com/Rajathbharadwaj/thrive-takehome.git
cd thrive-takehome
```

#### 2. **Deploy Everything (One Command!)**

```bash
# Deploy entire platform automatically
./scripts/setup.sh
```

**This script automatically:**
- âœ… Creates all AWS infrastructure (EKS, VPC, ECR, OIDC, IAM roles)
- âœ… Builds and pushes Docker image to ECR  
- âœ… Deploys Node.js application with health checks
- âœ… Sets up monitoring (Prometheus + Grafana)
- âœ… Configures Load Balancer and networking
- âœ… **No manual setup required!**

#### 3. **CI/CD is Ready!**

The setup script automatically configures CI/CD. GitHub Actions workflow will:
- âœ… **Auto-trigger** on pushes to `main` branch
- âœ… **Self-authenticate** with EKS cluster (no manual setup needed)  
- âœ… **Apply manifests** if deployment doesn't exist
- âœ… **Update application** with latest Docker image
- âœ… **Verify health** checks and endpoints

**GitHub Secrets Required** (add to repository settings):
```bash
AWS_ACCOUNT_ID: 123456789012  # Your AWS account ID
AWS_REGION: us-east-1         # Your AWS region
```

## ğŸ“Š Monitoring & Dashboards

### Access Monitoring Dashboards

```bash
# Start all monitoring dashboards in separate terminals
./scripts/start-dashboards.sh
```

**Available Dashboards:**
- **Application**: http://localhost:8080 (via ALB)
- **Prometheus**: http://localhost:9090 (metrics & targets)
- **Grafana**: http://localhost:3000 (admin/admin - visualization)
- **Argo Rollouts**: http://localhost:3100 (canary deployment status)

## ğŸŒ Live Application Access

The application is deployed and accessible at:

**ğŸ”— Production Application**: **http://thrive.docrag.io/**

### Available Endpoints

```bash
# Main application endpoint
curl http://thrive.docrag.io/

# Health check endpoint
curl http://thrive.docrag.io/health

# Prometheus metrics endpoint
curl http://thrive.docrag.io/metrics
```

### Response Examples

**Main Endpoint (`/`):**
```json
{
  "message": "Hello World from Thrive PRODUCTION! ğŸš€",
  "timestamp": "2025-08-15T09:24:37.650Z",
  "version": "1.0.0",
  "environment": "production",
  "hostname": "hello-world-rollout-99f7c47bc-7p6bv",
  "branch": "dev"
}
```

**Health Endpoint (`/health`):**
```json
{
  "status": "healthy",
  "timestamp": "2025-08-15T09:24:47.470Z",
  "uptime": 6714.4042195,
  "memory": {
    "rss": 70049792,
    "heapTotal": 19554304,
    "heapUsed": 14491616,
    "external": 1261707,
    "arrayBuffers": 49546
  },
  "pid": 19
}
```

### ğŸš¨ Alerting & Notifications

**CloudWatch Alarms:**
- **Location**: AWS Console > CloudWatch > Alarms
- **Email Setup**: Configure `alert_email` in `terraform/environments/prod.tfvars`
- **Automatic Triggers**:
  - ğŸ”´ High CPU utilization (>80% for 4 minutes)
  - ğŸ”´ Pod failures or crashes
  - ğŸ”´ Load balancer health check failures
- **Notification Method**: Email via SNS topic
- **Response Time**: < 5 minutes notification delivery

**Alert Categories:**
- **Infrastructure**: EKS cluster health, node status
- **Application**: Response time, error rates, availability
- **Resource**: CPU, memory, disk utilization

## ğŸš€ CI/CD Pipeline

### Single Production Environment

- **Production**: Push to `main` branch â†’ Deploys to `thrive-prod-cluster-fresh`
- **Automatic Setup**: CI/CD configures itself during `setup.sh` execution

### Pipeline Features

- âœ… **Smart Deployment**: Auto-applies manifests if deployment missing
- âœ… **Security**: OIDC authentication, no long-lived keys
- âœ… **Self-Healing**: Handles missing deployments gracefully  
- âœ… **Health Verification**: Automatic endpoint testing
- âœ… **Monitoring Integration**: Full observability stack

## ğŸ§ª Testing CI/CD Pipeline

```bash
# Test CI/CD by making a code change and pushing to main
echo "console.log('Updated via CI/CD!');" >> app/server.js
git add app/server.js
git commit -m "Test CI/CD deployment"
git push origin main

# Monitor deployment in GitHub Actions
# GitHub repository > Actions tab > Watch workflow progress
```

## ğŸ”§ Architecture Decisions & Tradeoffs

### **Infrastructure Choices**

| Component | Choice | Rationale | Tradeoff |
|-----------|---------|-----------|----------|
| **Container Orchestration** | Amazon EKS | Managed Kubernetes, enterprise-ready | Higher cost vs self-managed |
| **Load Balancer** | Application Load Balancer | Layer 7 routing, SSL termination | More complex than NLB |
| **Instance Type** | t3.small | Cost-effective for demo | Limited performance |
| **Node Count** | 2 nodes | High availability | Higher cost than single node |
| **Deployment Strategy** | Argo Rollouts | Advanced canary deployments | Additional complexity |
| **Monitoring** | Prometheus + Grafana | Industry standard, flexible | Self-managed vs CloudWatch |

### **Security Decisions**

- **OIDC Authentication**: No long-lived AWS keys in GitHub
- **IAM Roles**: Least privilege access with PowerUser + specific policies
- **Non-root Containers**: Security best practice
- **Private Subnets**: Database and application isolation
- **Secrets Management**: AWS Secrets Manager for sensitive data

### **Cost Optimization**

- **t3.small instances**: Balanced performance/cost
- **ECR lifecycle policies**: Automatic image cleanup
- **Development environment**: Minimal resources (1 node, t3.micro)
- **Auto-scaling**: Scale to zero during off-hours (configurable)

## ğŸ§¹ Cleanup

```bash
# Clean up all resources
./scripts/cleanup.sh

# Or destroy with Terraform
cd terraform && terraform destroy -auto-approve
```

## ğŸ“š Additional Resources

- **Kubectl Commands**: See [KUBECTL_COMMANDS.md](KUBECTL_COMMANDS.md)
- **Architecture Details**: See [ARCHITECTURE.md](ARCHITECTURE.md)
- **Canary Testing**: See [CANARY_TESTING_GUIDE.md](CANARY_TESTING_GUIDE.md)
- **Deployment Guide**: See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)
- **Troubleshooting**: Check GitHub Actions logs for deployment issues

---

**ğŸ¯ Total Deployment Time**: ~15-20 minutes  
**ğŸ”§ Components**: 25+ AWS resources, Kubernetes cluster, monitoring stack  
**ğŸ’° Estimated Cost**: ~$2-3/day for demo environment